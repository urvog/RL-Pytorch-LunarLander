{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20220706 Colab RL Practice DQN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNuuZghrGtHttpVfbtx1XL3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/urvog/RL-Pytorch-LunarLander/blob/main/20220706_Colab_RL_Practice_DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4CdPmqvP9lj",
        "outputId": "141b961a-e6a0-432f-d4b3-ea5a89bfa41d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting box2d-py\n",
            "  Downloading box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 6.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n"
          ]
        }
      ],
      "source": [
        "!pip3 install box2d-py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import base64, io\n",
        "import numpy as np\n",
        "from collections import deque, namedtuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "#For visualization\n",
        "from gym.wrappers.monitoring import video_recorder\n",
        "from IPython.display import HTML\n",
        "from IPython import display\n",
        "import glob"
      ],
      "metadata": {
        "id": "Bx0UG-2QP-Hp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"LunarLander-v2\")\n",
        "env.reset()\n",
        "print('Número de estados: ', env.observation_space.shape)\n",
        "print('Número de acciones: ',env.action_space.n)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0Ym2vgqP_de",
        "outputId": "625109d2-d076-410e-d64d-2c09c0af67d7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de estados:  (8,)\n",
            "Número de acciones:  4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class QNetwork(nn.Module):\n",
        "    \"\"\"Actor (Policy) Model.\"\"\"\n",
        "\n",
        "    def __init__(self, state_size, action_size, seed):\n",
        "        \"\"\"Initialize parameters and build model.\n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): Dimension de cada estado\n",
        "            action_size (int): Dimension de cada acción\n",
        "            seed (int): Random seed\n",
        "        \"\"\"\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        self.fc1 = nn.Linear(state_size, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.fc3 = nn.Linear(64, action_size)\n",
        "        \n",
        "    def forward(self, state):\n",
        "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
        "        x = self.fc1(state)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        return self.fc3(x)"
      ],
      "metadata": {
        "id": "8blkUErYQBrr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = int(1e5)  # tamaño del replay buffer\n",
        "BATCH_SIZE = 64         # tamaño de minibatch\n",
        "GAMMA = 0.99            # factor de descuento\n",
        "TAU = 1e-3              # para una actualizacion suave de los parametros\n",
        "LR = 5e-4               # learning rate \n",
        "UPDATE_EVERY = 4        # cada cuanto se actualizará la red"
      ],
      "metadata": {
        "id": "71J2RbeSQOfB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#mandamos a GPU\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "2eeS9xVfQSgu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent():\n",
        "    \"\"\"Interactúa y aprende del entorno.\"\"\"\n",
        "\n",
        "    def __init__(self, state_size, action_size, seed):\n",
        "        \"\"\"Inicializamos el objeto Agente\n",
        "        \n",
        "        Paramaetros\n",
        "        ======\n",
        "            state_size (int): dimension de cada estado\n",
        "            action_size (int): dimension de cada accion\n",
        "            seed (int): random seed\n",
        "        \"\"\"\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "        # Q-Network\n",
        "        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
        "        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
        "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
        "\n",
        "        # Replay memory\n",
        "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
        "        \n",
        "        # Inicializamos cada paso (para actualizacion cada paso asignado en UPDATE_EVERY)\n",
        "        self.t_step = 0\n",
        "    \n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        # Guardamos la experiencia en replay memory\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "        \n",
        "        # Aprende cada paso UPDATE_EVERY\n",
        "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
        "        if self.t_step == 0:\n",
        "            # Si cada muestra esta disponible en memory, obtenemos un conjunto aleatorio y aprendemos\n",
        "            if len(self.memory) > BATCH_SIZE:\n",
        "                experiences = self.memory.sample()\n",
        "                self.learn(experiences, GAMMA)\n",
        "\n",
        "    def act(self, state, eps=0.):\n",
        "        \"\"\"Devuelve accione dado un estado de acuerdo con su politica\n",
        "        \n",
        "        Parametros\n",
        "        ======\n",
        "            state (array_like): estado actual\n",
        "            eps (float): epsilon, para la seleccion de epsilon-greedy\n",
        "            \n",
        "        \"\"\"\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        self.qnetwork_local.eval()\n",
        "        with torch.no_grad():\n",
        "            action_values = self.qnetwork_local(state)\n",
        "        self.qnetwork_local.train()\n",
        "\n",
        "        # Seleccion de Epsilon-greedy\n",
        "        if random.random() > eps:\n",
        "            return np.argmax(action_values.cpu().data.numpy())\n",
        "        else:\n",
        "            return random.choice(np.arange(self.action_size))\n",
        "\n",
        "    def learn(self, experiences, gamma):\n",
        "        \"\"\"\"Actualizado los valores de los parametros dado un bacho de las tuplas de las experiencias.\n",
        "\n",
        "        Parametros\n",
        "        ======\n",
        "            experiences (Tuple[torch.Variable]): tupla de (s, a, r, s', done) tuplas\n",
        "            gamma (float): factor de descuento\n",
        "        \"\"\"\n",
        "        # Obtenemos un minibatch aleatorio de las tupcas \n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "        ## Procesamos y minimamos el LOSS\n",
        "        ### Extraemos el maximo valor estimado de la red\n",
        "        q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "        \n",
        "        ### Calculamos el valor target utilizando la ecuacion de BELLMAN\n",
        "        \n",
        "        q_targets = rewards + gamma * q_targets_next * (1 - dones)\n",
        "        \n",
        "        ### Calculamos los valores esperados de nuestra red\n",
        "        q_expected = self.qnetwork_local(states).gather(1, actions)\n",
        "        \n",
        "        ### Calculamos LOSS empleando MSE\n",
        "        \n",
        "        loss = F.mse_loss(q_expected, q_targets)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        #Actualizamos la RED\n",
        "        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)                     \n",
        "\n",
        "    def soft_update(self, local_model, target_model, tau):\n",
        "        \"\"\"Parametros de Soft update model\n",
        "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
        "\n",
        "        Parameetros\n",
        "        ======\n",
        "            local_model (PyTorch model): pesos que seran copiados de \n",
        "            target_model (PyTorch model): pesos que seran copiados a \n",
        "            tau (float): parametro de interpolación \n",
        "        \"\"\"\n",
        "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
      ],
      "metadata": {
        "id": "FdqhakuDQZGi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    \"\"\"Tamaño fijo para almacenar las experiencias de las tuplas.\"\"\"\n",
        "\n",
        "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
        "        \"\"\"Iniciliazamos el objeto ReplayBuffer.\n",
        "\n",
        "        Parametros\n",
        "        ======\n",
        "            action_size (int): dimension de cada acción\n",
        "            buffer_size (int): tamaño maximo del buffer\n",
        "            batch_size (int): tamaño de cada batach para el entrenamiento\n",
        "            seed (int): random seed\n",
        "        \"\"\"\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=buffer_size)  \n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "        self.seed = random.seed(seed)\n",
        "    \n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Añadimos una nueva experiencia a la memoria.\"\"\"\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(e)\n",
        "    \n",
        "    def sample(self):\n",
        "        \"\"\"Aleatoriamente seleccionamos un batch de las experiencias en memoria.\"\"\"\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
        "  \n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Devuelve el tamaño actual de la memoria interna.\"\"\"\n",
        "        return len(self.memory)"
      ],
      "metadata": {
        "id": "ht6tgK1vQfTv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
        "    \"\"\"Deep Q-Learning.\n",
        "    \n",
        "    Params\n",
        "    ======\n",
        "        n_episodes (int): numero máximo de episodios\n",
        "        max_t (int): número maximo de pasos por episodio\n",
        "        eps_start (float): valor inicial de epsilon, para la selección de epsilon-greedy\n",
        "        eps_end (float): mínimo valor de epsilon\n",
        "        eps_decay (float): factor (per episode) para reducir el valor epsilon\n",
        "    \"\"\"\n",
        "    scores = []                        # list que contiene los scores de cada episodio\n",
        "    scores_window = deque(maxlen=100)  # ultimos 100 scores\n",
        "    eps = eps_start                    # inicializamos epsilon\n",
        "    for i_episode in range(1, n_episodes+1):\n",
        "        state = env.reset()\n",
        "        score = 0\n",
        "        for t in range(max_t):\n",
        "            action = agent.act(state, eps)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            score += reward\n",
        "            if done:\n",
        "                break \n",
        "        scores_window.append(score)       # guardamos los ultimos score\n",
        "        scores.append(score)              # guardamos los ultimos score\n",
        "        eps = max(eps_end, eps_decay*eps) # reducimos epsilon\n",
        "        print('\\rEpisode {}\\tValor Promedio Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
        "        if i_episode % 100 == 0:\n",
        "            print('\\rEpisode {}\\tValor Promedio: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
        "        if np.mean(scores_window)>=200.0:\n",
        "            print('\\nEntorno resuelto in {:d} episodes!\\tValor Promedio: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
        "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
        "            break\n",
        "    return scores"
      ],
      "metadata": {
        "id": "A3mx8KZcQhQD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "493ffa80-a22d-4796-ae04-9d3e7ad34739",
        "outputId": "dc14066d-8328-41c1-d942-f5400d813ab9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 100\tValor Promedio: -169.02\n",
            "Episode 200\tValor Promedio: -140.30\n",
            "Episode 300\tValor Promedio: -7.11\n",
            "Episode 364\tValor Promedio Score: -22.49"
          ]
        }
      ],
      "source": [
        "#ENTRENAMOS LA RED\n",
        "agent = Agent(state_size=8, action_size=4, seed=0)\n",
        "\n",
        "#PARAMETROS PARA EL ENTRENAMIENTO\n",
        "n_episodes = 1000\n",
        "max_t = 1000\n",
        "eps_end = 0.01\n",
        "eps_decay = 0.995\n",
        "\n",
        "scores = dqn(n_episodes=n_episodes, max_t=max_t, eps_end=eps_end, eps_decay=eps_decay)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RyYKENUVQsP7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}